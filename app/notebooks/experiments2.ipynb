{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcd6f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23157b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a256b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.services.RAG_service import RAGService\n",
    "\n",
    "rag = RAGService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3914ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.load_and_split_document(type=\"pdf\", path = r\"app\\notebooks\\KshitijResume.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rag.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6823b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b8643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d0dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "print(HfFolder.get_token())\n",
    "token = HfFolder.get_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## downloading gemma3 270m from huggingface\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"google/gemma-3-270m-it\", token = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"hi? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56f5770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b8e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"google/gemma-3-270m\",\n",
    "    task=\"text-generation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c499a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872638b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Translate English to French: {text}\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "\n",
    "# Create a LangChain chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Run the chain\n",
    "input_text = \"\"\n",
    "response = chain.invoke({\"text\": \"Can you give me structured outputs ?\"})\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef1f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Replace with your actual Space URL\n",
    "url = \"https://kshitijk20-ollama.hf.space/api/generate\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gemma3:270m\",\n",
    "    \"prompt\": \"What are the benefits of small language models?\",\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "# print(response.json().get(\"response\"))\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837b840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {hf_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gemma3:270m\",\n",
    "    \"prompt\": \"Hi\",\n",
    "    \"stream\": False  # Set to False for simple testing\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# CRITICAL: Print the text to see if the error is from Hugging Face or Ollama\n",
    "if response.status_code == 404:\n",
    "    print(\"Error Details:\", response.text)\n",
    "else:\n",
    "    print(response.json().get(\"response\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0556bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import os\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=\"https://kshitijk20-ollama.hf.space\",\n",
    "    model=\"gemma3:270m\",\n",
    "    headers={\n",
    "        \"Authorization\": f\"Bearer {HF_TOKEN}\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(llm.invoke(\"hi\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc26b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import os\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=\"https://kshitijk20-ollama.hf.space\",\n",
    "    model=\"gemma3:1b\",\n",
    "    headers={\n",
    "        \"Authorization\": f\"Bearer {HF_TOKEN}\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(llm.invoke(\"hi\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840722c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm2 = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    ")\n",
    "llm.invoke(\"hi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc0075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the model with OpenRouter's base URL\n",
    "llm3 = init_chat_model(\n",
    "    model=\"google/gemma-3-27b-it:free\",\n",
    "    model_provider=\"openai\",\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=getenv(\"OPENROUTER_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "response = model.invoke(\"hi?\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bfbfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb8fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350052e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.utils.model_loader import ModelLoader\n",
    "from app.ingestion.file_loader import FileLoader\n",
    "from app.ingestion.text_splitter import splitting_text\n",
    "from app.retrieval.retriever import Retriever\n",
    "from app.embedding.embeder import QueryEmbedding\n",
    "from app.embedding.vectore_store import VectorStore\n",
    "from app.metadata_extraction.metadata_ext import MetadataExtractor\n",
    "from app.utils.metadata_utils import MetadataService\n",
    "from langchain_core.documents import Document\n",
    "import json\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Global model instances (loaded once)\n",
    "_embedding_model = None\n",
    "\n",
    "def get_models():\n",
    "    global  _embedding_model\n",
    "    if _embedding_model is None:\n",
    "        print(\"Loading models (one-time initialization)...\")\n",
    "        embedding_loader = ModelLoader(model_provider=\"huggingface\")\n",
    "        _embedding_model = embedding_loader.load_llm()\n",
    "    return _embedding_model\n",
    "\n",
    "class RAGService: \n",
    "    def __init__(self):\n",
    "        print(\"[RAGService] Initializing service...\")\n",
    "        self._init_models()\n",
    "        self.Docuement_Type = None \n",
    "        self.Pinecone_index = None\n",
    "        self.Document_path = None\n",
    "        self.Document_Type = None\n",
    "        self.DocumentTypeScheme = None\n",
    "        self.url = None\n",
    "        self.chunks = None\n",
    "        self.vector_store = None\n",
    "        self.index = None\n",
    "        self.namespace = None\n",
    "        self.retriever = None\n",
    "        self.metadataservice = MetadataService()\n",
    "        print(\"[RAGService] Initialization complete.\")\n",
    "\n",
    "    def _init_models(self):\n",
    "        \"\"\"Initialize LLM and embedding Models\"\"\"\n",
    "        print(\"[RAGService] Loading LLM model (openrouter)...\")\n",
    "        self.model_loader = ModelLoader(model_provider=\"openrouter\")\n",
    "        self.llm = self.model_loader.load_llm()\n",
    "        # self.llm = llm3\n",
    "        print(\"[RAGService] LLM model loaded.\")\n",
    "        print(\"[RAGService] Loading embedding model (huggingface)...\")\n",
    "        # self.model_loader = ModelLoader(model_provider=\"huggingface\")\n",
    "        self.embedding_model = get_models()\n",
    "        print(\"[RAGService] Embedding model loaded.\")\n",
    "\n",
    "    def load_and_split_document(self, type:str, path:str= None, url:str = None):\n",
    "        \"\"\"Load and chunk document from local path or URL\"\"\"\n",
    "        print(f\"[RAGService] Loading document. Type: {type}, Path: {path}, URL: {url}\")\n",
    "        file_loader = FileLoader(llm = self.llm)\n",
    "        if type == \"pdf\":\n",
    "            if path:\n",
    "                print(f\"[RAGService] Loading PDF from path: {path}\")\n",
    "                doc = file_loader.load_pdf(path)\n",
    "            elif url:\n",
    "                print(f\"[RAGService] Loading PDF from URL: {url}\")\n",
    "                doc = file_loader.load_documents_from_url(url)\n",
    "            else:\n",
    "                print(\"[RAGService] Error: Either path or url must be provided for PDF.\")\n",
    "                raise ValueError(\"Either path or url must be provided for PDF.\")\n",
    "        elif type == \"word\":\n",
    "            if path:\n",
    "                print(f\"[RAGService] Loading Word document from path: {path}\")\n",
    "                doc = file_loader.load_word_document(path)\n",
    "            elif url:\n",
    "                print(\"[RAGService] Error: URL loading not supported for Word documents.\")\n",
    "                raise ValueError(\"URL loading not supported for Word documents.\")\n",
    "            else:\n",
    "                print(\"[RAGService] Error: Path must be provided for Word document.\")\n",
    "                raise ValueError(\"Path must be provided for Word document.\")\n",
    "        else:\n",
    "            print(\"[RAGService] Error: Unsupported document type.\")\n",
    "            raise ValueError(\"Unsupported document type. Use 'pdf' or 'word'.\")\n",
    "        \n",
    "        print(\"[RAGService] Detecting document type scheme...\")\n",
    "        self.DocumentTypeScheme = file_loader.detect_document_type(doc[0:2])\n",
    "        print(f\"[RAGService] Document type scheme detected: {self.DocumentTypeScheme}\")\n",
    "        self.Document_Type = self.metadataservice.Return_document_model(self.DocumentTypeScheme)\n",
    "        print(f\"[RAGService] Document type model: {self.Document_Type}\")\n",
    "        ## \n",
    "        from datetime import datetime\n",
    "        self.splitter = splitting_text(documentTypeSchema=self.Document_Type, llm=self.llm, embedding_model=self.embedding_model)\n",
    "        print(\"[RAGService] Splitting document into chunks...\")\n",
    "        start_time = datetime.now()\n",
    "        self.chunks = self.splitter.text_splitting(doc)\n",
    "        end_time = datetime.now()\n",
    "        print(f\"[RAGService] Time taken to extract metadata with splitter: {end_time - start_time}\")\n",
    "        print(f\"[RAGService] Total chunks created: {len(self.chunks)}\")\n",
    "\n",
    "    def create_query_embedding(self, query: str):\n",
    "        print(\"[RAGService] Creating query embedding...\")\n",
    "        self.query = query\n",
    "        self.query_embedder = QueryEmbedding(query=query, embedding_model=self.embedding_model)\n",
    "        self.query_embedding = self.query_embedder.get_embedding()\n",
    "        print(f\"[RAGService] Query embedding created: {self.query_embedding}\")\n",
    "        langchain_doc = Document(page_content=query)\n",
    "        print(\"[RAGService] Extracting metadata for the query...\")\n",
    "        self.metadataExtractor = MetadataExtractor(llm=self.llm)\n",
    "        with open(self.splitter.Keywordsfile_path, \"r\") as f:\n",
    "            known_keywords = json.load(f)\n",
    "        raw_metadata = self.metadataExtractor.extractMetadata_query(self.Document_Type,langchain_doc, known_keywords = known_keywords)\n",
    "        print(f\"[RAGService] Query metadata extracted: {raw_metadata}\")\n",
    "        # Convert to dictionary and format for Pinecone\n",
    "        metadata_dict = raw_metadata.model_dump(exclude_none=True)\n",
    "        formatted_metadata = self.metadataservice.format_metadata_for_pinecone(metadata_dict)\n",
    "        \n",
    "        # Remove problematic fields that cause serialization issues\n",
    "        self.query_metadata = {\n",
    "            k: v for k, v in formatted_metadata.items() \n",
    "            if k not in [\"obligations\", \"exclusions\", \"notes\", \"added_new_keyword\"]\n",
    "        }\n",
    "    \n",
    "        print(f\"[RAGService] Query metadata type: {type(self.query_metadata)}\")\n",
    "        print(f\"[RAGService] Query metadata: {self.query_metadata}\")\n",
    "\n",
    "    def create_vector_store(self):\n",
    "        print(\"[RAGService] Creating vector store...\")\n",
    "        self.vector_store_class_instance = VectorStore(self.chunks, self.embedding_model)\n",
    "        self.index, self.namespace, self.vector_store = self.vector_store_class_instance.create_vectorestore()\n",
    "        print(f\"[RAGService] Vector store created. Index: {self.index}, Namespace: {self.namespace}\")\n",
    "        ### Sparse Retriever(BM25)\n",
    "        self.sparse_retriever=BM25Retriever.from_documents(self.chunks)\n",
    "        self.sparse_retriever.k=3 ##top- k documents to retriever\n",
    "\n",
    "        \n",
    "\n",
    "    def retrive_documents(self, raw_query: str):\n",
    "        print(\"[RAGService] Retrieving documents from vector store...\")\n",
    "        self.create_query_embedding(raw_query)\n",
    "        \n",
    "        self.retriever = Retriever(self.index,raw_query,self.query_metadata, self.namespace, self.vector_store,sparse_retriever = self.sparse_retriever,llm = self.llm)\n",
    "        self.result = self.retriever.retrieval_from_pinecone_vectoreStore()\n",
    "        # self.result = self.retriever.invoke(raw_query)\n",
    "        # print(f\"[RAGService] Retrieval result: {self.result}\")\n",
    "    \n",
    "    def answer_query(self, raw_query:str) -> str:\n",
    "        \"\"\"Answer user query using retrieved documents and LLM\"\"\"\n",
    "        print(f\"[RAGService] Answering query: {raw_query}\")\n",
    "        # top_clause = self.result['matches']\n",
    "        # top_clause_dicts = [r.to_dict() for r in top_clause]\n",
    "        # self.top_clauses = top_clause_dicts\n",
    "        # keys_to_remove = {\"file_path\", \"source\", \"producer\", \"keywords\", \"subject\", \"added_new_keyword\", \"author\", \"chunk_id\"}\n",
    "        # for r in top_clause_dicts:\n",
    "        #     meta = r.get(\"metadata\", {})\n",
    "        #     for k in keys_to_remove:\n",
    "        #         meta.pop(k, None)\n",
    "\n",
    "        # context_clauses = json.dumps(top_clause_dicts, separators=(\",\", \":\"))\n",
    "        context_clauses = [doc.page_content for doc in self.result]\n",
    "\n",
    "        print(f\"context_clauses: {context_clauses}\")\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a legal/insurance domain expert and policy analyst. \n",
    "        Use the following extracted clauses from policy documents to answer the question.  \n",
    "        If you can't find the answer, say \"I don't know\".\n",
    "        Context clauses:\n",
    "        {\"\".join(context_clauses)}\n",
    "        Question: {raw_query}\n",
    "        \"\"\"\n",
    "        print(\"[RAGService] Invoking LLM with prompt...\")\n",
    "        response = self.llm.invoke(prompt)\n",
    "        print(f\"[RAGService] LLM response: {response}\")\n",
    "        \n",
    "        # Extract string content from response object\n",
    "        if hasattr(response, 'content'):\n",
    "            return response.content\n",
    "        elif isinstance(response, str):\n",
    "            return response\n",
    "        else:\n",
    "            return str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be28e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAGService()\n",
    "rag.load_and_split_document(type=\"pdf\", path = r\"app\\notebooks\\KshitijResume.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453dc099",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag.chunks[1].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5311e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = \"\"\"National Parivar Mediclaim Plus Policy\n",
    "Whereas the Proposer designated in the schedule hereto has by a Proposal together with Declaration, which shall be the basis of\n",
    "this contract and is deemed to be incorporated herein, has applied to National Insurance Company Ltd. (hereinafter called the\n",
    "Company), for the insurance hereinafter set forth, in respect of person(s)/ family members named in the schedule hereto\n",
    "(hereinafter called the Insured Persons) and has paid the premium as consideration for such insurance.\n",
    "1 PREAMBLE\n",
    "The Company undertakes that if during the Policy Period, any Insured Person shall suffer any illness or disease (hereinafter called\n",
    "Illness) or sustain any bodily injury due to an Accident (hereinafter called Injury) requiring Hospitalisation of such Insured\n",
    "Person(s) for In-Patient Care at any hospital/nursing home (hereinafter called Hospital) or for Day Care Treatment at any Day\n",
    "Care Center or to undergo treatment under Domiciliary Hospitalisation, following the Medical Advice of a duly qualified Medical\n",
    "Practitioner, the Company shall indemnify the Hospital or the Insured, Reasonable and Customary Charges incurred for Medically\n",
    "Necessary Treatment towards the Coverage mentioned herein.\n",
    "Provided further that, the amount payable under the Policy in respect of all such claims during each Policy Year of the Policy\n",
    "Period shall be subject to the Definitions, Terms, Exclusions, Conditions contained herein and limits as shown in the Table of\n",
    "Benefits, and shall not exceed the Floater Sum Insured in respect of the Insured family.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec62602c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.4 environment at: C:\\code\\Claridoc\\Rag_app\\.venv\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m43 packages\u001b[0m \u001b[2min 5.85s\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m thinc \u001b[2m(1.4MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m numpy \u001b[2m(14.8MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m en-core-web-sm \u001b[2m(12.2MiB)\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m en-core-web-sm\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m thinc\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m numpy\n",
      "\u001b[2mPrepared \u001b[1m5 packages\u001b[0m \u001b[2min 9.96s\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m4 packages\u001b[0m \u001b[2min 2.28s\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m8 packages\u001b[0m \u001b[2min 2.00s\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mblis\u001b[0m\u001b[2m==1.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mblis\u001b[0m\u001b[2m==0.7.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1men-core-web-sm\u001b[0m\u001b[2m==3.7.1 (from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangcodes\u001b[0m\u001b[2m==3.5.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mshellingham\u001b[0m\u001b[2m==1.5.4\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mspacy\u001b[0m\u001b[2m==3.8.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mspacy\u001b[0m\u001b[2m==3.7.5\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mthinc\u001b[0m\u001b[2m==8.3.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mthinc\u001b[0m\u001b[2m==8.2.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.21.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f63185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=C:\\code\\Bajaj HackRx\\Rag_app\\.venv` does not match the project environment path `c:\\code\\Claridoc\\Rag_app\\.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "C:\\code\\Claridoc\\Rag_app\\.venv\\Scripts\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "!uv run python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "868752e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dcdbc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1efcce195e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4179af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e21bd77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = {\n",
    "        \"candidate_phrases\": [],\n",
    "        \"parties\": [],\n",
    "        \"jurisdiction\": []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    phrase = chunk.text.strip()\n",
    "    if len(phrase) > 2 and not phrase.lower().startswith((\"this\", \"those\", \"that\", \"these\")):\n",
    "        out[\"candidate_phrases\"].append(phrase)\n",
    "    \n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f3b79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# how many documents processed\n",
    "TOTAL_DOCS = 0\n",
    "\n",
    "# phrase -> in how many docs it appeared\n",
    "PHRASE_DOC_FREQ = defaultdict(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0f9f173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'National Parivar Mediclaim Plus Policy\\nWhereas the Proposer designated in the schedule hereto has by a Proposal together with Declaration, which shall be the basis of\\nthis contract and is deemed to be incorporated herein, has applied to National Insurance Company Ltd. (hereinafter called the\\nCompany), for the insurance hereinafter set forth, in respect of person(s)/ family members named in the schedule hereto\\n(hereinafter called the Insured Persons) and has paid the premium as consideration for such insurance.\\n1 PREAMBLE\\nThe Company undertakes that if during the Policy Period, any Insured Person shall suffer any illness or disease (hereinafter called\\nIllness) or sustain any bodily injury due to an Accident (hereinafter called Injury) requiring Hospitalisation of such Insured\\nPerson(s) for In-Patient Care at any hospital/nursing home (hereinafter called Hospital) or for Day Care Treatment at any Day\\nCare Center or to undergo treatment under Domiciliary Hospitalisation, following the Medical Advice of a duly qualified Medical\\nPractitioner, the Company shall indemnify the Hospital or the Insured, Reasonable and Customary Charges incurred for Medically\\nNecessary Treatment towards the Coverage mentioned herein.\\nProvided further that, the amount payable under the Policy in respect of all such claims during each Policy Year of the Policy\\nPeriod shall be subject to the Definitions, Terms, Exclusions, Conditions contained herein and limits as shown in the Table of\\nBenefits, and shall not exceed the Floater Sum Insured in respect of the Insured family.\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "222e5787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def extract_candidate_phrases(text: str):\n",
    "    doc = nlp(text)\n",
    "    phrases = []\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        head = chunk.root\n",
    "\n",
    "        # ---- syntactic strength filter ----\n",
    "        if head.pos_ not in {\"NOUN\", \"PROPN\"}:\n",
    "            continue\n",
    "\n",
    "        phrase = \" \".join(chunk.text.split()).lower()\n",
    "\n",
    "        # length sanity\n",
    "        if len(phrase) < 4:\n",
    "            continue\n",
    "\n",
    "        phrases.append(phrase)\n",
    "\n",
    "    return list(set(phrases))\n",
    "def update_phrase_stats(phrases: list[str]):\n",
    "    global TOTAL_DOCS\n",
    "    TOTAL_DOCS += 1\n",
    "\n",
    "    for p in set(phrases):\n",
    "        PHRASE_DOC_FREQ[p] += 1\n",
    "def is_informative_phrase(phrase: str, max_doc_ratio: float = 0.3):\n",
    "    \"\"\"\n",
    "    Drops phrases that appear in too many documents.\n",
    "    Equivalent to low-IDF phrases.\n",
    "    \"\"\"\n",
    "    if TOTAL_DOCS == 0:\n",
    "        return True\n",
    "\n",
    "    doc_ratio = PHRASE_DOC_FREQ[phrase] / TOTAL_DOCS\n",
    "    return doc_ratio <= max_doc_ratio\n",
    "def filter_phrases_for_embeddings(phrases: list[str]):\n",
    "    filtered = []\n",
    "\n",
    "    for p in phrases:\n",
    "        if is_informative_phrase(p):\n",
    "            filtered.append(p)\n",
    "\n",
    "    return filtered\n",
    "def process_chunk(text: str):\n",
    "    # 1. extract\n",
    "    phrases = extract_candidate_phrases(text)\n",
    "\n",
    "    # 2. update corpus stats\n",
    "    update_phrase_stats(phrases)\n",
    "\n",
    "    # 3. filter\n",
    "    final_phrases = filter_phrases_for_embeddings(phrases)\n",
    "\n",
    "    return final_phrases\n",
    "print(process_chunk(chunk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63d3a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "def spacy_extract_from_chunk(text: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract candidate phrases + entities from ONE chunk using spaCy.\n",
    "    This is meant to run before embeddings.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    out = {\n",
    "        \"candidate_phrases\": [],\n",
    "        \"parties\": [],\n",
    "        \"jurisdiction\": []\n",
    "    }\n",
    "\n",
    "    return doc \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1️⃣ Candidate phrases (noun chunks) ----\n",
    "    for chunk in doc.noun_chunks:\n",
    "        phrase = chunk.text.strip()\n",
    "        # light cleaning\n",
    "        if len(phrase) > 2 and not phrase.lower().startswith((\"this\", \"that\", \"these\", \"those\")):\n",
    "            out[\"candidate_phrases\"].append(phrase)\n",
    "\n",
    "    # ---- 2️⃣ Named entities ----\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in (\"ORG\", \"PERSON\"):\n",
    "            out[\"parties\"].append(ent.text)\n",
    "        elif ent.label_ in (\"GPE\", \"LOC\"):\n",
    "            out[\"jurisdiction\"].append(ent.text)\n",
    "\n",
    "    # ---- de-duplicate ----\n",
    "    for k in out:\n",
    "        out[k] = list(set(out[k]))\n",
    "\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
