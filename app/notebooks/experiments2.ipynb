{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcd6f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23157b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a256b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.services.RAG_service import RAGService\n",
    "\n",
    "rag = RAGService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3914ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.load_and_split_document(type=\"pdf\", path = r\"app\\notebooks\\KshitijResume.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rag.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6823b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b8643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d0dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "print(HfFolder.get_token())\n",
    "token = HfFolder.get_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## downloading gemma3 270m from huggingface\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"google/gemma-3-270m-it\", token = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"hi? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56f5770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b8e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"google/gemma-3-270m\",\n",
    "    task=\"text-generation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c499a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872638b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Translate English to French: {text}\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "\n",
    "# Create a LangChain chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Run the chain\n",
    "input_text = \"\"\n",
    "response = chain.invoke({\"text\": \"Can you give me structured outputs ?\"})\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef1f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Replace with your actual Space URL\n",
    "url = \"https://kshitijk20-ollama.hf.space/api/generate\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gemma3:270m\",\n",
    "    \"prompt\": \"What are the benefits of small language models?\",\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "# print(response.json().get(\"response\"))\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837b840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {hf_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gemma3:270m\",\n",
    "    \"prompt\": \"Hi\",\n",
    "    \"stream\": False  # Set to False for simple testing\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# CRITICAL: Print the text to see if the error is from Hugging Face or Ollama\n",
    "if response.status_code == 404:\n",
    "    print(\"Error Details:\", response.text)\n",
    "else:\n",
    "    print(response.json().get(\"response\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0556bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import os\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=\"https://kshitijk20-ollama.hf.space\",\n",
    "    model=\"gemma3:270m\",\n",
    "    headers={\n",
    "        \"Authorization\": f\"Bearer {HF_TOKEN}\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(llm.invoke(\"hi\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc26b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import os\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=\"https://kshitijk20-ollama.hf.space\",\n",
    "    model=\"gemma3:1b\",\n",
    "    headers={\n",
    "        \"Authorization\": f\"Bearer {HF_TOKEN}\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(llm.invoke(\"hi\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840722c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm2 = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    ")\n",
    "llm.invoke(\"hi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc0075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the model with OpenRouter's base URL\n",
    "llm3 = init_chat_model(\n",
    "    model=\"google/gemma-3-27b-it:free\",\n",
    "    model_provider=\"openai\",\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=getenv(\"OPENROUTER_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "response = model.invoke(\"hi?\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bfbfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb8fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350052e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.utils.model_loader import ModelLoader\n",
    "from app.ingestion.file_loader import FileLoader\n",
    "from app.ingestion.text_splitter import splitting_text\n",
    "from app.retrieval.retriever import Retriever\n",
    "from app.embedding.embeder import QueryEmbedding\n",
    "from app.embedding.vectore_store import VectorStore\n",
    "from app.metadata_extraction.metadata_ext import MetadataExtractor\n",
    "from app.utils.metadata_utils import MetadataService\n",
    "from langchain_core.documents import Document\n",
    "import json\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Global model instances (loaded once)\n",
    "_embedding_model = None\n",
    "\n",
    "def get_models():\n",
    "    global  _embedding_model\n",
    "    if _embedding_model is None:\n",
    "        print(\"Loading models (one-time initialization)...\")\n",
    "        embedding_loader = ModelLoader(model_provider=\"huggingface\")\n",
    "        _embedding_model = embedding_loader.load_llm()\n",
    "    return _embedding_model\n",
    "\n",
    "class RAGService: \n",
    "    def __init__(self):\n",
    "        print(\"[RAGService] Initializing service...\")\n",
    "        self._init_models()\n",
    "        self.Docuement_Type = None \n",
    "        self.Pinecone_index = None\n",
    "        self.Document_path = None\n",
    "        self.Document_Type = None\n",
    "        self.DocumentTypeScheme = None\n",
    "        self.url = None\n",
    "        self.chunks = None\n",
    "        self.vector_store = None\n",
    "        self.index = None\n",
    "        self.namespace = None\n",
    "        self.retriever = None\n",
    "        self.metadataservice = MetadataService()\n",
    "        print(\"[RAGService] Initialization complete.\")\n",
    "\n",
    "    def _init_models(self):\n",
    "        \"\"\"Initialize LLM and embedding Models\"\"\"\n",
    "        print(\"[RAGService] Loading LLM model (openrouter)...\")\n",
    "        self.model_loader = ModelLoader(model_provider=\"openrouter\")\n",
    "        self.llm = self.model_loader.load_llm()\n",
    "        # self.llm = llm3\n",
    "        print(\"[RAGService] LLM model loaded.\")\n",
    "        print(\"[RAGService] Loading embedding model (huggingface)...\")\n",
    "        # self.model_loader = ModelLoader(model_provider=\"huggingface\")\n",
    "        self.embedding_model = get_models()\n",
    "        print(\"[RAGService] Embedding model loaded.\")\n",
    "\n",
    "    def load_and_split_document(self, type:str, path:str= None, url:str = None):\n",
    "        \"\"\"Load and chunk document from local path or URL\"\"\"\n",
    "        print(f\"[RAGService] Loading document. Type: {type}, Path: {path}, URL: {url}\")\n",
    "        file_loader = FileLoader(llm = self.llm)\n",
    "        if type == \"pdf\":\n",
    "            if path:\n",
    "                print(f\"[RAGService] Loading PDF from path: {path}\")\n",
    "                doc = file_loader.load_pdf(path)\n",
    "            elif url:\n",
    "                print(f\"[RAGService] Loading PDF from URL: {url}\")\n",
    "                doc = file_loader.load_documents_from_url(url)\n",
    "            else:\n",
    "                print(\"[RAGService] Error: Either path or url must be provided for PDF.\")\n",
    "                raise ValueError(\"Either path or url must be provided for PDF.\")\n",
    "        elif type == \"word\":\n",
    "            if path:\n",
    "                print(f\"[RAGService] Loading Word document from path: {path}\")\n",
    "                doc = file_loader.load_word_document(path)\n",
    "            elif url:\n",
    "                print(\"[RAGService] Error: URL loading not supported for Word documents.\")\n",
    "                raise ValueError(\"URL loading not supported for Word documents.\")\n",
    "            else:\n",
    "                print(\"[RAGService] Error: Path must be provided for Word document.\")\n",
    "                raise ValueError(\"Path must be provided for Word document.\")\n",
    "        else:\n",
    "            print(\"[RAGService] Error: Unsupported document type.\")\n",
    "            raise ValueError(\"Unsupported document type. Use 'pdf' or 'word'.\")\n",
    "        \n",
    "        print(\"[RAGService] Detecting document type scheme...\")\n",
    "        self.DocumentTypeScheme = file_loader.detect_document_type(doc[0:2])\n",
    "        print(f\"[RAGService] Document type scheme detected: {self.DocumentTypeScheme}\")\n",
    "        self.Document_Type = self.metadataservice.Return_document_model(self.DocumentTypeScheme)\n",
    "        print(f\"[RAGService] Document type model: {self.Document_Type}\")\n",
    "        ## \n",
    "        from datetime import datetime\n",
    "        self.splitter = splitting_text(documentTypeSchema=self.Document_Type, llm=self.llm, embedding_model=self.embedding_model)\n",
    "        print(\"[RAGService] Splitting document into chunks...\")\n",
    "        start_time = datetime.now()\n",
    "        self.chunks = self.splitter.text_splitting(doc)\n",
    "        end_time = datetime.now()\n",
    "        print(f\"[RAGService] Time taken to extract metadata with splitter: {end_time - start_time}\")\n",
    "        print(f\"[RAGService] Total chunks created: {len(self.chunks)}\")\n",
    "\n",
    "    def create_query_embedding(self, query: str):\n",
    "        print(\"[RAGService] Creating query embedding...\")\n",
    "        self.query = query\n",
    "        self.query_embedder = QueryEmbedding(query=query, embedding_model=self.embedding_model)\n",
    "        self.query_embedding = self.query_embedder.get_embedding()\n",
    "        print(f\"[RAGService] Query embedding created: {self.query_embedding}\")\n",
    "        langchain_doc = Document(page_content=query)\n",
    "        print(\"[RAGService] Extracting metadata for the query...\")\n",
    "        self.metadataExtractor = MetadataExtractor(llm=self.llm)\n",
    "        with open(self.splitter.Keywordsfile_path, \"r\") as f:\n",
    "            known_keywords = json.load(f)\n",
    "        raw_metadata = self.metadataExtractor.extractMetadata_query(self.Document_Type,langchain_doc, known_keywords = known_keywords)\n",
    "        print(f\"[RAGService] Query metadata extracted: {raw_metadata}\")\n",
    "        # Convert to dictionary and format for Pinecone\n",
    "        metadata_dict = raw_metadata.model_dump(exclude_none=True)\n",
    "        formatted_metadata = self.metadataservice.format_metadata_for_pinecone(metadata_dict)\n",
    "        \n",
    "        # Remove problematic fields that cause serialization issues\n",
    "        self.query_metadata = {\n",
    "            k: v for k, v in formatted_metadata.items() \n",
    "            if k not in [\"obligations\", \"exclusions\", \"notes\", \"added_new_keyword\"]\n",
    "        }\n",
    "    \n",
    "        print(f\"[RAGService] Query metadata type: {type(self.query_metadata)}\")\n",
    "        print(f\"[RAGService] Query metadata: {self.query_metadata}\")\n",
    "\n",
    "    def create_vector_store(self):\n",
    "        print(\"[RAGService] Creating vector store...\")\n",
    "        self.vector_store_class_instance = VectorStore(self.chunks, self.embedding_model)\n",
    "        self.index, self.namespace, self.vector_store = self.vector_store_class_instance.create_vectorestore()\n",
    "        print(f\"[RAGService] Vector store created. Index: {self.index}, Namespace: {self.namespace}\")\n",
    "        ### Sparse Retriever(BM25)\n",
    "        self.sparse_retriever=BM25Retriever.from_documents(self.chunks)\n",
    "        self.sparse_retriever.k=3 ##top- k documents to retriever\n",
    "\n",
    "        \n",
    "\n",
    "    def retrive_documents(self, raw_query: str):\n",
    "        print(\"[RAGService] Retrieving documents from vector store...\")\n",
    "        self.create_query_embedding(raw_query)\n",
    "        \n",
    "        self.retriever = Retriever(self.index,raw_query,self.query_metadata, self.namespace, self.vector_store,sparse_retriever = self.sparse_retriever,llm = self.llm)\n",
    "        self.result = self.retriever.retrieval_from_pinecone_vectoreStore()\n",
    "        # self.result = self.retriever.invoke(raw_query)\n",
    "        # print(f\"[RAGService] Retrieval result: {self.result}\")\n",
    "    \n",
    "    def answer_query(self, raw_query:str) -> str:\n",
    "        \"\"\"Answer user query using retrieved documents and LLM\"\"\"\n",
    "        print(f\"[RAGService] Answering query: {raw_query}\")\n",
    "        # top_clause = self.result['matches']\n",
    "        # top_clause_dicts = [r.to_dict() for r in top_clause]\n",
    "        # self.top_clauses = top_clause_dicts\n",
    "        # keys_to_remove = {\"file_path\", \"source\", \"producer\", \"keywords\", \"subject\", \"added_new_keyword\", \"author\", \"chunk_id\"}\n",
    "        # for r in top_clause_dicts:\n",
    "        #     meta = r.get(\"metadata\", {})\n",
    "        #     for k in keys_to_remove:\n",
    "        #         meta.pop(k, None)\n",
    "\n",
    "        # context_clauses = json.dumps(top_clause_dicts, separators=(\",\", \":\"))\n",
    "        context_clauses = [doc.page_content for doc in self.result]\n",
    "\n",
    "        print(f\"context_clauses: {context_clauses}\")\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a legal/insurance domain expert and policy analyst. \n",
    "        Use the following extracted clauses from policy documents to answer the question.  \n",
    "        If you can't find the answer, say \"I don't know\".\n",
    "        Context clauses:\n",
    "        {\"\".join(context_clauses)}\n",
    "        Question: {raw_query}\n",
    "        \"\"\"\n",
    "        print(\"[RAGService] Invoking LLM with prompt...\")\n",
    "        response = self.llm.invoke(prompt)\n",
    "        print(f\"[RAGService] LLM response: {response}\")\n",
    "        \n",
    "        # Extract string content from response object\n",
    "        if hasattr(response, 'content'):\n",
    "            return response.content\n",
    "        elif isinstance(response, str):\n",
    "            return response\n",
    "        else:\n",
    "            return str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be28e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAGService()\n",
    "rag.load_and_split_document(type=\"pdf\", path = r\"app\\notebooks\\KshitijResume.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453dc099",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag.chunks[1].metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
